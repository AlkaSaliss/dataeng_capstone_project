{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 95%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 95%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOPRO2TS : LOst PROperties Tracking Service in Train Stations\n",
    "\n",
    "## Problem specification\n",
    "\n",
    "A new startup is willing to create an app to allow users to track lost properties in different trains/stations in France, by providing them a AI-empowered step-by-step guidance based on property's characteristics, train station characteristics, etc. Therefore, their first object is to do a market research.<br>\n",
    "To acquire a deep understanding of this mysterious unexplored phenomenon üëΩ of lost properties in trains and/or stations, they decided to recruit the famous Data Engineer who helped the **Sparkify** startup to become N¬∞1 in the world : Me üßê!!! <br>\n",
    "My mission this time is to create an ETL, _uh uhm_, **ELT** pipeline to create a more analytics-friendly version of the dataset they intend to use.\n",
    "\n",
    "They decided to use some open datasets provided by the main French transportation service : [SNCF Group](https://www.sncf.com/en), and also temperature data provided by www.weathernews.fr.\n",
    "\n",
    "\n",
    "## Datasets description\n",
    "\n",
    "### List of French train stations  \n",
    "This dataset is a json file with around 4K records. It lists french train stations and some of their characteristics. The dataset is updated on a yearly basis and can be downloaded [here](https://ressources.data.sncf.com/explore/dataset/referentiel-gares-voyageurs/download/?format=json&timezone=Europe/Berlin&lang=fr).<br>\n",
    "Example of a record in the json file: \n",
    "```json\n",
    "{'datasetid': 'referentiel-gares-voyageurs',\n",
    " 'fields': {'adresse_cp': '93140',\n",
    "            'alias_libelle_noncontraint': 'Remise √† Jorelle',\n",
    "            'code': '00002-1',\n",
    "            'commune_code': '010',\n",
    "            'commune_libellemin': 'Bondy',\n",
    "            'departement_libellemin': 'Seine-Saint-Denis',\n",
    "            'departement_numero': '93',\n",
    "            'gare': '{\"DRG_ON\": true, \"Etrangere_ON\": false, \"NbPltf\": 1, '\n",
    "                    '\"Alias_Libelle_NonContraint\": \"Remise √† Jorelle\", '\n",
    "                    '\"Alias_Libelle_Fronton\": \"Remise √† Jorelle\", '\n",
    "                    '\"AgenceGC_Libelle\": \"Direction G√©n√©rale des Gares '\n",
    "                    '√éle-de-France\", \"RegionSNCF_Libelle\": \"REGION DE '\n",
    "                    'PARIS-EST\", \"UG_Libelle\": null, \"UT_Libelle\": \"BONDY GARE '\n",
    "                    'REMISE A JORELLE TRAM TRAIN\"}',\n",
    "            'gare_agencegc_libelle': 'Direction G√©n√©rale des Gares '\n",
    "                                     '√éle-de-France',\n",
    "            'gare_alias_libelle_fronton': 'Remise √† Jorelle',\n",
    "            'gare_alias_libelle_noncontraint': 'Remise √† Jorelle',\n",
    "            'gare_drg_on': 'True',\n",
    "            'gare_etrangere_on': 'False',\n",
    "            'gare_nbpltf': 1,\n",
    "            'gare_regionsncf_libelle': 'REGION DE PARIS-EST',\n",
    "            'gare_ut_libelle': 'BONDY GARE REMISE A JORELLE TRAM TRAIN',\n",
    "            'latitude_entreeprincipale_wgs84': '48.893170',\n",
    "            'longitude_entreeprincipale_wgs84': '2.487751',\n",
    "            'rg_libelle': 'Gare La Remise √† Jorelle',\n",
    "            'segmentdrg_libelle': 'b',\n",
    "            'tvs': 'RJL',\n",
    "            'tvss': '[{\"TVS_Code\": \"RJL\"}]',\n",
    "            'uic_code': '0087988709',\n",
    "            'wgs_84': [48.89317, 2.487751]},\n",
    " 'geometry': {'coordinates': [2.487751, 48.89317], 'type': 'Point'},\n",
    " 'record_timestamp': '2020-12-29T00:00:51.658+01:00',\n",
    " 'recordid': 'fbaead07f41e47e2c3cc424e43e92972f898b740'}\n",
    "```\n",
    "The most interesting field for us is `fields` with the following sub-fields :\n",
    "* **uic_code** : train station identification code\n",
    "* **alias_libelle_noncontraint** : train station name\n",
    "* **latitude_entreeprincipale_wgs84** and **longitude_entreeprincipale_wgs84** : latitude and longitude resp. of the station main entrance (based on  World Geodetic System 1984)\n",
    "* **commune_code** and **commune_libellemin** : code and name of city where station is located\n",
    "* **departement_numero** and **departement_libellemin** : code and name of the county where the station is located\n",
    "* **adresse_cp** : postal code\n",
    "* **gare_nbpltf** : number of platforms in station\n",
    "* **segmentdrg_libelle** : station category encode as `a`, `b` or `c`, depend on the traffic coverage (nationnal/international, regional, local) and yearly number of passengers\n",
    "\n",
    "\n",
    "### Lost properties declaration dataset \n",
    "\n",
    "This dataset is a csv with around 1.14M records. It contains information on loss declarations made by customers through various media (website, app, in person to SNCF staff, ...).  The dataset is updated on a daily basis and can be found [here](https://ressources.data.sncf.com/explore/dataset/objets-trouves-gares/download/?format=csv&timezone=Europe/Berlin&lang=fr&use_labels_for_header=true&csv_separator=%3B).<br>\n",
    "This dataset is updated on a daily basis.<br>\n",
    "Following are the list of the dataset's columns :\n",
    "* **Date** : date of loss declaration by customer\n",
    "* **Gare** : Name of the train station where the loss was declared\n",
    "* **Code UIC** : train station identification code\n",
    "* **Type d'objets** : category of the property (e.g. category can be `electronics` , `clothes`, ...)\n",
    "* **Nature d'objets** : a more fine-grained category of the property (e.g. `mobile phone`, `scarf`, ...)\n",
    "\n",
    "* **Type d'enregistrement** : recording type. Currently it has only one value : `D√©claration de Perte` (declaration of loss)\n",
    "\n",
    "\n",
    "### Found properties dataset \n",
    "\n",
    "This dataset is a csv with around 700K records. It contains information on lost properties that have been declared by customers and found in trains/stations by staff. The data tells if the property was recovered by its owner. The dataset is updated on a daily basis and can be found [here](https://ressources.data.sncf.com/explore/dataset/objets-trouves-restitution/download/?format=csv&timezone=Europe/Berlin&lang=fr&use_labels_for_header=true&csv_separator=%3B).<br>\n",
    "Following are the list of the dataset's columns :\n",
    "* **Date** : it is unclear whether this is loss declaration date, or date when the property was found by the staff. Therefore we'll consider it as loss declaration date\n",
    "* **Date et heure de restitution** : if property was recovered by its owner, this is the date of recovery\n",
    "* **Gare** : Name of the train station where the property was found\n",
    "* **Code UIC** : train station identification code\n",
    "* **Type d'objets** : category of the property (e.g. category can be `electronics` , `clothes`, ...)\n",
    "* **Nature d'objets** : a more fine-grained category of the property (e.g. `mobile phone`, `scarf`, ...)\n",
    "* **Type d'enregistrement** : recording type. Currently it has only one value : `Objet trouv√©` (Found object)\n",
    "\n",
    "\n",
    "### Daily temperatures dataset \n",
    "\n",
    "70% of economic activities are sensitive to temperature data. Also cognitive abilities are shown to be impacted by temperature ([source](https://www.cambridgebrainsciences.com/more/articles/as-temperature-goes-up-cognitive-performance-goes-down#:~:text=A%20hot%20environment%20has%20been,impaired%20more%20than%20simple%20tasks.)). For example uncomfortable heat can diminish cognitive abilities. Thus it seem interesting to include temperature data in our pipeline to hep refine future analysis.\n",
    "\n",
    "This dataset is a csv with around 100K records. It contains information on daily temperatures for each county in France since 2018. The dataset is updated on a monthly basis and can be found [here](https://opendata.reseaux-energies.fr/explore/dataset/temperature-quotidienne-departementale/download/?format=csv&timezone=Europe/Berlin&lang=fr&use_labels_for_header=true&csv_separator=%3B).<br>\n",
    "Following are the list of the dataset's columns :\n",
    "* **Date** : temperature measurement date\n",
    "* **Code INSEE d√©partement** : county code\n",
    "* **D√©partement** : county name\n",
    "* **TMin (¬∞C)** : daily minimum temperature in degre celsius\n",
    "* **TMax (¬∞C)** : daily maximum temperature in degre celsius\n",
    "* **TMoy (¬∞C)** : daily average temperature in degre celsius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "In this section we'll do a preliminary data exploration to have an idea of the various datasets content.<br>\n",
    "Let's first import some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. List of train stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 86.9 ms, sys: 19.8 ms, total: 107 ms\n",
      "Wall time: 825 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load data\n",
    "with open(\"input_data/referentiel-gares-voyageurs.json\") as f:\n",
    "    train_stations = json.load(f)\n",
    "list_colums = [\"uic_code\", \"alias_libelle_noncontraint\", \"latitude_entreeprincipale_wgs84\", \"longitude_entreeprincipale_wgs84\",\n",
    "               \"commune_code\", \"commune_libellemin\", \"departement_numero\", \"departement_libellemin\", \"adresse_cp\", \"gare_nbpltf\", \"segmentdrg_libelle\"]\n",
    "df_stations = pd.DataFrame([item[\"fields\"] for item in train_stations])[list_colums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uic_code</th>\n",
       "      <th>alias_libelle_noncontraint</th>\n",
       "      <th>latitude_entreeprincipale_wgs84</th>\n",
       "      <th>longitude_entreeprincipale_wgs84</th>\n",
       "      <th>commune_code</th>\n",
       "      <th>commune_libellemin</th>\n",
       "      <th>departement_numero</th>\n",
       "      <th>departement_libellemin</th>\n",
       "      <th>adresse_cp</th>\n",
       "      <th>gare_nbpltf</th>\n",
       "      <th>segmentdrg_libelle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0087988709</td>\n",
       "      <td>Remise √† Jorelle</td>\n",
       "      <td>48.893170</td>\n",
       "      <td>2.487751</td>\n",
       "      <td>010</td>\n",
       "      <td>Bondy</td>\n",
       "      <td>93</td>\n",
       "      <td>Seine-Saint-Denis</td>\n",
       "      <td>93140</td>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0087784892</td>\n",
       "      <td>Bena Fanes</td>\n",
       "      <td>42.4580782</td>\n",
       "      <td>1.9167264</td>\n",
       "      <td>066</td>\n",
       "      <td>Enveitg</td>\n",
       "      <td>66</td>\n",
       "      <td>Pyr√©n√©es-Orientales</td>\n",
       "      <td>66760</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0087784769</td>\n",
       "      <td>Fontp√©drouse</td>\n",
       "      <td>42.5138062</td>\n",
       "      <td>2.1886585</td>\n",
       "      <td>080</td>\n",
       "      <td>Fontp√©drouse</td>\n",
       "      <td>66</td>\n",
       "      <td>Pyr√©n√©es-Orientales</td>\n",
       "      <td>66360</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0087784736</td>\n",
       "      <td>Nyer</td>\n",
       "      <td>42.5416979</td>\n",
       "      <td>2.2630177</td>\n",
       "      <td>123</td>\n",
       "      <td>Nyer</td>\n",
       "      <td>66</td>\n",
       "      <td>Pyr√©n√©es-Orientales</td>\n",
       "      <td>66360</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0087784686</td>\n",
       "      <td>Villefranche - Vernet-les-Bains</td>\n",
       "      <td>42.591998</td>\n",
       "      <td>2.370396</td>\n",
       "      <td>223</td>\n",
       "      <td>Villefranche-de-Conflent</td>\n",
       "      <td>66</td>\n",
       "      <td>Pyr√©n√©es-Orientales</td>\n",
       "      <td>66820</td>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     uic_code       alias_libelle_noncontraint  \\\n",
       "0  0087988709                 Remise √† Jorelle   \n",
       "1  0087784892                       Bena Fanes   \n",
       "2  0087784769                     Fontp√©drouse   \n",
       "3  0087784736                             Nyer   \n",
       "4  0087784686  Villefranche - Vernet-les-Bains   \n",
       "\n",
       "  latitude_entreeprincipale_wgs84 longitude_entreeprincipale_wgs84  \\\n",
       "0                       48.893170                         2.487751   \n",
       "1                      42.4580782                        1.9167264   \n",
       "2                      42.5138062                        2.1886585   \n",
       "3                      42.5416979                        2.2630177   \n",
       "4                       42.591998                         2.370396   \n",
       "\n",
       "  commune_code        commune_libellemin departement_numero  \\\n",
       "0          010                     Bondy                 93   \n",
       "1          066                   Enveitg                 66   \n",
       "2          080              Fontp√©drouse                 66   \n",
       "3          123                      Nyer                 66   \n",
       "4          223  Villefranche-de-Conflent                 66   \n",
       "\n",
       "  departement_libellemin adresse_cp  gare_nbpltf segmentdrg_libelle  \n",
       "0      Seine-Saint-Denis      93140            1                  b  \n",
       "1    Pyr√©n√©es-Orientales      66760            1                  c  \n",
       "2    Pyr√©n√©es-Orientales      66360            1                  c  \n",
       "3    Pyr√©n√©es-Orientales      66360            1                  c  \n",
       "4    Pyr√©n√©es-Orientales      66820            1                  b  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of train stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have : 2867 train stations in total\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have : {len(df_stations)} train stations in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uic_code                            0\n",
       "alias_libelle_noncontraint          0\n",
       "latitude_entreeprincipale_wgs84     4\n",
       "longitude_entreeprincipale_wgs84    4\n",
       "commune_code                        0\n",
       "commune_libellemin                  0\n",
       "departement_numero                  0\n",
       "departement_libellemin              0\n",
       "adresse_cp                          0\n",
       "gare_nbpltf                         0\n",
       "segmentdrg_libelle                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stations.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stations.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicates based on code station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stations.duplicated([\"uic_code\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lost properties declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 221 ms, total: 13.6 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load data\n",
    "df_lost = pd.read_csv(\"input_data/objets-trouves-gares.csv\", sep=\";\", dtype={\"Code UIC\": str})\n",
    "# convert date string to datetime\n",
    "_map_date = {dt: datetime.strptime(dt[:19], \"%Y-%m-%dT%H:%M:%S\") for dt in df_lost.Date.unique() if pd.notnull(dt)}\n",
    "df_lost[\"Date\"] = df_lost.Date.map(_map_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Gare</th>\n",
       "      <th>Code UIC</th>\n",
       "      <th>Nature d'objets</th>\n",
       "      <th>Type d'objets</th>\n",
       "      <th>Type d'enregistrement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-02-13 23:08:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Autres divers</td>\n",
       "      <td>Divers</td>\n",
       "      <td>D√©claration de Perte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-02-13 23:22:13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Livre scolaire</td>\n",
       "      <td>Livres, articles de pap√©terie</td>\n",
       "      <td>D√©claration de Perte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-02-13 23:24:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Livre de poche</td>\n",
       "      <td>Livres, articles de pap√©terie</td>\n",
       "      <td>D√©claration de Perte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-02-14 07:07:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sac √† dos</td>\n",
       "      <td>Bagagerie: sacs, valises, cartables</td>\n",
       "      <td>D√©claration de Perte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-02-14 07:54:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ordinateur, ordinateur portable, notebook</td>\n",
       "      <td>Appareils √©lectroniques, informatiques, appare...</td>\n",
       "      <td>D√©claration de Perte</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date Gare Code UIC  \\\n",
       "0 2019-02-13 23:08:31  NaN      NaN   \n",
       "1 2019-02-13 23:22:13  NaN      NaN   \n",
       "2 2019-02-13 23:24:16  NaN      NaN   \n",
       "3 2019-02-14 07:07:30  NaN      NaN   \n",
       "4 2019-02-14 07:54:09  NaN      NaN   \n",
       "\n",
       "                             Nature d'objets  \\\n",
       "0                              Autres divers   \n",
       "1                             Livre scolaire   \n",
       "2                             Livre de poche   \n",
       "3                                  Sac √† dos   \n",
       "4  Ordinateur, ordinateur portable, notebook   \n",
       "\n",
       "                                       Type d'objets Type d'enregistrement  \n",
       "0                                             Divers  D√©claration de Perte  \n",
       "1                      Livres, articles de pap√©terie  D√©claration de Perte  \n",
       "2                      Livres, articles de pap√©terie  D√©claration de Perte  \n",
       "3                Bagagerie: sacs, valises, cartables  D√©claration de Perte  \n",
       "4  Appareils √©lectroniques, informatiques, appare...  D√©claration de Perte  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lost.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have : 1,148,442 loss declarations in total\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have : {len(df_lost):,} loss declarations in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The earliest record in the dataset is on 2013-05-24, and the latest record on 2020-12-23\n"
     ]
    }
   ],
   "source": [
    "print(f\"The earliest record in the dataset is on {df_lost.Date.min().date()}, and the latest record on {df_lost.Date.max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lost.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can uniquely identify a loss declaration using `Date`, `station code`, `object nature` and `object type` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lost.duplicated([\"Date\", \"Code UIC\", \"Nature d'objets\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Even though we don't have dupicates using these columns in the current dataset, there is no guarantee that in the future two different declarations can't be done at the same for the same object, in the same station --> we need to use another identifier like auto-increment integer when building data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missng values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                          0\n",
       "Gare                     786193\n",
       "Code UIC                 786193\n",
       "Nature d'objets               0\n",
       "Type d'objets                 0\n",
       "Type d'enregistrement         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lost.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Seems there are a lot of missing train stations. A potential explanation is that a property that is lost in a train during a trip will not be linked to any station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Found properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 s, sys: 308 ms, total: 12.1 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load data\n",
    "df_found = pd.read_csv(\"input_data/objets-trouves-restitution.csv\", sep=\";\", dtype={\"Code UIC\": str})\n",
    "# convert date string to datetime\n",
    "_map_date = {dt: datetime.strptime(dt[:19], \"%Y-%m-%dT%H:%M:%S\") for dt in set(df_found.Date).union(set(df_found[\"Date et heure de restitution\"])) if pd.notnull(dt)}\n",
    "df_found[\"Date\"] = df_found.Date.map(_map_date)\n",
    "df_found[\"Date et heure de restitution\"] = df_found[\"Date et heure de restitution\"].map(_map_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Date et heure de restitution</th>\n",
       "      <th>Gare</th>\n",
       "      <th>Code UIC</th>\n",
       "      <th>Nature d'objets</th>\n",
       "      <th>Type d'objets</th>\n",
       "      <th>Type d'enregistrement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-23 14:33:43</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Bourges</td>\n",
       "      <td>0087576207</td>\n",
       "      <td>Autres divers (pr√©ciser)</td>\n",
       "      <td>Divers</td>\n",
       "      <td>Objet trouv√©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-03-10 09:07:15</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Toulouse Matabiau</td>\n",
       "      <td>0087611004</td>\n",
       "      <td>Sac de voyage, sac de sport, sac √† bandouli√®re</td>\n",
       "      <td>Bagagerie: sacs, valises, cartables</td>\n",
       "      <td>Objet trouv√©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-02-06 08:45:32</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Toulouse Matabiau</td>\n",
       "      <td>0087611004</td>\n",
       "      <td>Sac √† dos</td>\n",
       "      <td>Bagagerie: sacs, valises, cartables</td>\n",
       "      <td>Objet trouv√©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-03-10 09:14:38</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Paris Est</td>\n",
       "      <td>0087113001</td>\n",
       "      <td>Manteau, veste, blazer, parka, blouson, cape</td>\n",
       "      <td>V√™tements, chaussures</td>\n",
       "      <td>Objet trouv√©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-02-06 09:01:24</td>\n",
       "      <td>2014-02-07 20:55:39</td>\n",
       "      <td>Paris Est</td>\n",
       "      <td>0087113001</td>\n",
       "      <td>Valise, sac sur roulettes</td>\n",
       "      <td>Bagagerie: sacs, valises, cartables</td>\n",
       "      <td>Objet trouv√©</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date Date et heure de restitution               Gare  \\\n",
       "0 2018-01-23 14:33:43                          NaT            Bourges   \n",
       "1 2014-03-10 09:07:15                          NaT  Toulouse Matabiau   \n",
       "2 2014-02-06 08:45:32                          NaT  Toulouse Matabiau   \n",
       "3 2014-03-10 09:14:38                          NaT          Paris Est   \n",
       "4 2014-02-06 09:01:24          2014-02-07 20:55:39          Paris Est   \n",
       "\n",
       "     Code UIC                                 Nature d'objets  \\\n",
       "0  0087576207                        Autres divers (pr√©ciser)   \n",
       "1  0087611004  Sac de voyage, sac de sport, sac √† bandouli√®re   \n",
       "2  0087611004                                       Sac √† dos   \n",
       "3  0087113001    Manteau, veste, blazer, parka, blouson, cape   \n",
       "4  0087113001                       Valise, sac sur roulettes   \n",
       "\n",
       "                         Type d'objets Type d'enregistrement  \n",
       "0                               Divers          Objet trouv√©  \n",
       "1  Bagagerie: sacs, valises, cartables          Objet trouv√©  \n",
       "2  Bagagerie: sacs, valises, cartables          Objet trouv√©  \n",
       "3                V√™tements, chaussures          Objet trouv√©  \n",
       "4  Bagagerie: sacs, valises, cartables          Objet trouv√©  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_found.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have : 705,747 declared and found properties in total\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have : {len(df_found):,} declared and found properties in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The earliest record in the dataset is on 2013-05-24, and the latest record on 2020-12-23\n"
     ]
    }
   ],
   "source": [
    "print(f\"The earliest record in the dataset is on {df_found.Date.min().date()}, and the latest record on {df_found.Date.max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The earliest lost property recovered in the dataset is on 2013-05-24, and the latest on 2020-12-23\n"
     ]
    }
   ],
   "source": [
    "print(f\"The earliest lost property recovered in the dataset is on {df_found['Date et heure de restitution'].min().date()}, and the latest on {df_found['Date et heure de restitution'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_found.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can uniquely identify a loss declaration using `Date`, `station code`, `object nature` and `object type` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58253"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_found.duplicated([\"Date\", \"Code UIC\", \"Nature d'objets\", \"Type d'objets\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using these columns doesnt' allow us to uniquely identify a record in the dataset --> we'll need to create our own id, such as an auto-increment integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missng values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                                 0\n",
       "Date et heure de restitution    493471\n",
       "Gare                               299\n",
       "Code UIC                           299\n",
       "Nature d'objets                      0\n",
       "Type d'objets                        0\n",
       "Type d'enregistrement                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_found.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Seems there are some missing train stations. A potential explanation is that a property that is lost in a train during a trip will not be linked to any station.\n",
    "\n",
    "> There are also a lot of missing recovery dates, corresponding most likely to properties that haven't been recovered yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Daily temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 116 ms, sys: 11.9 ms, total: 128 ms\n",
      "Wall time: 127 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load data\n",
    "df_temperature = pd.read_csv(\"input_data/temperature-quotidienne-departementale.csv\", sep=\";\")\n",
    "# convert date string to datetime\n",
    "_map_date = {dt: datetime.strptime(dt, \"%Y-%m-%d\") for dt in df_temperature.Date.unique()}\n",
    "df_temperature[\"Date\"] = df_temperature.Date.map(_map_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Code INSEE d√©partement</th>\n",
       "      <th>D√©partement</th>\n",
       "      <th>TMin (¬∞C)</th>\n",
       "      <th>TMax (¬∞C)</th>\n",
       "      <th>TMoy (¬∞C)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>87</td>\n",
       "      <td>Haute-Vienne</td>\n",
       "      <td>2.30</td>\n",
       "      <td>10.00</td>\n",
       "      <td>6.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>19</td>\n",
       "      <td>Corr√®ze</td>\n",
       "      <td>-2.70</td>\n",
       "      <td>10.90</td>\n",
       "      <td>4.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>83</td>\n",
       "      <td>Var</td>\n",
       "      <td>4.77</td>\n",
       "      <td>14.08</td>\n",
       "      <td>9.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>27</td>\n",
       "      <td>Eure</td>\n",
       "      <td>-3.10</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>23</td>\n",
       "      <td>Creuse</td>\n",
       "      <td>-4.50</td>\n",
       "      <td>13.20</td>\n",
       "      <td>4.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date Code INSEE d√©partement   D√©partement  TMin (¬∞C)  TMax (¬∞C)  \\\n",
       "0 2019-12-31                     87  Haute-Vienne       2.30      10.00   \n",
       "1 2019-12-31                     19       Corr√®ze      -2.70      10.90   \n",
       "2 2019-12-31                     83           Var       4.77      14.08   \n",
       "3 2019-12-31                     27          Eure      -3.10       5.50   \n",
       "4 2019-12-31                     23        Creuse      -4.50      13.20   \n",
       "\n",
       "   TMoy (¬∞C)  \n",
       "0       6.15  \n",
       "1       4.10  \n",
       "2       9.43  \n",
       "3       1.20  \n",
       "4       4.35  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-12-31 00:00:00')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature.Date[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have : 102,240 temperature records in total\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have : {len(df_temperature):,} temperature records in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The earliest record in the dataset is on 2018-01-01, and the latest record on 2020-11-30\n"
     ]
    }
   ],
   "source": [
    "print(f\"The earliest record in the dataset is on {df_temperature.Date.min().date()}, and the latest record on {df_temperature.Date.max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can uniquely identify a loss declaration using `Date`, `county code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature.duplicated([\"Date\", \"Code INSEE d√©partement\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using these columns allows us to uniquely identify a record in the temperature dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missng values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                      0\n",
       "Code INSEE d√©partement    0\n",
       "D√©partement               0\n",
       "TMin (¬∞C)                 0\n",
       "TMax (¬∞C)                 0\n",
       "TMoy (¬∞C)                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> No missing Values in temperatures dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data model\n",
    "\n",
    "The ideal situation would be to join the two datasets, `Lost properties declaration dataset` and `Found properties dataset`, so that we can see which object was lost when and where, whether it was found, and if/when it was recovered by its owner. and this will be used as our fact table.<br>Unfortunately there's currently no id that allows us to identify a record in either dataset. We could've used a collection of columns (`Date`, `station code`, `object nature` and `object type`), but there are duplicates in the `Found properties dataset` using these fields.\n",
    "\n",
    "Finally we decided to create two fact tables in our datalake and two dimension tables.\n",
    "\n",
    "The two dimension tables are :\n",
    "* time : contains date and time related information. The column names of this table are self-explanatory (see the star schema below)\n",
    "* stations : contains information related to train stations. The column names of this table are self-explanatory (see the star schema below). We'll add a default station to take into account declarations that are not related to any station. This default station will have `NOT_A_STATION` value for all non-numeric columns and `-999` for numeric columns. The `station_category` column encodes the type of station using three levels :\n",
    "    * \"a\" : more than 250,000 yearly passengers\n",
    "    * \"b\" : more than 100,000 yearly passengers\n",
    "    * \"c\" : other stations\n",
    "\n",
    "The two fact tables are :\n",
    "* declared_loss : contains information on all loss declaration made by customers through website, app, or in-person. The columns are :\n",
    "    * declaration_id : auto-incremented integer to identify each customer declaration\n",
    "    * date_and_time : date and time of the declaration\n",
    "    * station_id : station identification code\n",
    "    * property_type : a coarse categorisation of loss properties as explained earlier in the introduction. e.g. `V√™tements, chaussures` (clothes, shoes), `Appareils √©lectroniques, informatiques, appareils photo` (electronic devices), ...\n",
    "    * property_nature : a more fine-grained categorisation of loss properties as explained earlier in the introduction. e.g. `Manteau, veste, blazer, parka, blouson, cape` (Coat, jacket), `T√©l√©phone portable` (smartphone), ...\n",
    "    * min_temperature : the minimum temperature recorded this day in the county where station is located\n",
    "    * max_temperature : the maximum temperature recorded this day in the county where station is located\n",
    "    * avg_temperature : the average temperature recorded this day in the county where station is located\n",
    "* declared_and_found_properties : contains information on all lost and found properties declared by customers through app. The columns are :\n",
    "    * found_id : auto-incremented integer to identify each found property\n",
    "    * date_and_time : date and time of the declaration\n",
    "    * station_id : station identification code\n",
    "    * property_type : same as in the previous table\n",
    "    * property_nature : same as in previous table\n",
    "    * recovery_date : for properties that have been returned to their owners, this is the recovery date and time\n",
    "    * delay_before_recovery : for properties that have been returned to their owners, this is the delay in hours between the loss declaration and recovery date\n",
    "    * min_temperature : the minimum temperature recorded this day in the county where station is located\n",
    "    * max_temperature : the maximum temperature recorded this day in the county where station is located\n",
    "    * avg_temperature : the average temperature recorded this day in the county where station is located\n",
    "    \n",
    "![model](assests/data_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data model should allwo us to focus on properties loss declaration / recovery and be able to make interesting business related analysis using dimensions like time, stations, property type, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project setup\n",
    "\n",
    "The project structure is the following:\n",
    "![folder](assests/folder.png)\n",
    "\n",
    "* `utils.py` script contains some helper function such as text formatting, csv and json files reading functions with spark, ...\n",
    "* `data_quality_tests.py` script contains some data quality tests that are applied to our different tables\n",
    "* `etl.py` is the main script and is used for running the ETL pipeline\n",
    "\n",
    "\n",
    "### 1. Run the ELT pipeline\n",
    "\n",
    "The script `etl.py` contains all is needed to run the project. In the `input_data` folder we have all the project related datasets : \n",
    "* `objets-trouves-gares.csv` : loss declaration data\n",
    "* `objets-trouves-restitution.csv` : lost and found properties data\n",
    "* `temperature-quotidienne-departementale.csv` : counties temperature data\n",
    "* `referentiel-gares-voyageurs.json` : train stations data\n",
    "\n",
    "The pipeline uses pyspark to process the data, so you'll need it nstall to run the project.\n",
    "To execute the ELT processing pipeline simply run :\n",
    "```sh\n",
    "python etl.py\n",
    "```\n",
    "\n",
    "This will extract, load, transform, and finally save the data in the fact and dimension tables in parquet files.The logs are written to `app.log` file and also printed on terminal. The output tables are saved in a newly created folder `output_data`.\n",
    "> The tables `loss_declaration` and `declared_and_found` are both partitioned by `station_id` and `property_type` because it provides a good balance between computational performance and business logic.\n",
    "\n",
    "> `time` table is partitioned by `year` and `month`\n",
    "\n",
    "\n",
    "### 2. Data Quality Checks \n",
    "\n",
    "The following quality check functions are applied :\n",
    "* `check_unique_key` : checks if primary key is unique, applied to all tables\n",
    "* `check_non_empty` : checks if table is not empty, applied to all tables\n",
    "* `check_min` : checks that values of a given column are greater than a given threshold, e.g. number of platform are always positive for a train station\n",
    "* `check_range` : for two columns, checks that values of first column are always lower than values of the second column; when this is not the case the values of second column are set to values of first column \n",
    "    * minimum temperature should be lower than maximum temperature\n",
    "    * loss property found date should be earlier than recovery date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Some business analytics from our data model\n",
    "\n",
    "In this section we'll perform some analytical queries against our data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sp_conf = (SparkConf().set(\"spark.sql.sources.partitionColumnTypeInference.enabled\", False))\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(conf=sp_conf)\\\n",
    "        .appName(\"LOPRO2TS\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tables\n",
    "\n",
    "df_stations = spark.read.parquet(\"output_data/stations/\")\n",
    "df_time = spark.read.parquet(\"output_data/time/\")\n",
    "df_loss_declaration = spark.read.parquet(\"output_data/loss_declaration/\")\n",
    "df_declared_and_found = spark.read.parquet(\"output_data/declared_and_found/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2868, 1954980, 1148442, 705747)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check non empty\n",
    "df_stations.count(), df_time.count(), df_loss_declaration.count(), df_declared_and_found.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Top 10 stations that recorded lost properties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|        station_name|count|\n",
      "+--------------------+-----+\n",
      "|Paris Gare de Lyo...|40814|\n",
      "|Paris Montparnass...|34079|\n",
      "|  Paris Gare du Nord|27123|\n",
      "|  Paris Saint-Lazare|25421|\n",
      "|          Strasbourg|17576|\n",
      "| Bordeaux Saint-Jean|16248|\n",
      "|           Paris Est|14036|\n",
      "|        Lille Europe|12991|\n",
      "|      Lyon Part Dieu|10521|\n",
      "|       Lyon Perrache| 9804|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_loss_declaration\\\n",
    "    .join(df_stations, \"station_id\", \"left\")\\\n",
    "    .where(\"station_id != 'NOT_A_STATION'\")\\\n",
    "    .groupBy(\"station_name\")\\\n",
    "    .count()\\\n",
    "    .orderBy(F.desc(\"count\"))\\\n",
    "    .limit(10)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems top 4 stations are from Paris, which is understandable given it is the capital city and has the highest traffic in France."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Top years that recorded lost properties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|year| count|\n",
      "+----+------+\n",
      "|2017|192929|\n",
      "|2019|185067|\n",
      "|2015|181099|\n",
      "|2016|177243|\n",
      "|2018|174956|\n",
      "|2020|114192|\n",
      "|2014|114103|\n",
      "|2013|  8853|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_loss_declaration\\\n",
    "    .join(df_time, \"date_and_time\", \"left\")\\\n",
    "    .groupBy(\"year\")\\\n",
    "    .count()\\\n",
    "    .orderBy(F.desc(\"count\"))\\\n",
    "    .limit(10)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Top stations by recovery rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------------+\n",
      "|station_name            |recovery_rate(%)  |\n",
      "+------------------------+------------------+\n",
      "|Bellegarde-sur-Valserine|60.947867298578196|\n",
      "|Chalon-sur-Sa√¥ne        |54.716981132075475|\n",
      "|Bourg-en-Bresse         |53.25581395348837 |\n",
      "|Dieppe                  |50.442477876106196|\n",
      "|Angoul√™me               |48.538011695906434|\n",
      "|Coutances               |48.148148148148145|\n",
      "|Vannes                  |47.91208791208791 |\n",
      "|Redon                   |47.337278106508876|\n",
      "|Arras                   |46.200980392156865|\n",
      "|Yvetot                  |45.6140350877193  |\n",
      "|Boulogne-sur-Mer        |45.213726670680316|\n",
      "|Valenciennes            |45.205479452054796|\n",
      "|Cerb√®re                 |44.067796610169495|\n",
      "|Aulnoye-Aymeries        |41.666666666666664|\n",
      "|Belfort                 |41.48936170212766 |\n",
      "|Maubeuge                |41.46341463414634 |\n",
      "|Carcassonne             |40.388768898488124|\n",
      "|Blois - Chambord        |40.22082018927445 |\n",
      "|Laval                   |40.06849315068493 |\n",
      "|Vichy                   |40.0              |\n",
      "+------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "station_count = df_declared_and_found\\\n",
    "    .join(df_stations, \"station_id\", \"left\")\\\n",
    "    .where(\"station_id != 'NOT_A_STATION'\")\\\n",
    "    .groupBy(\"station_name\")\\\n",
    "    .agg(F.count(\"station_name\").alias(\"count_station\"))\n",
    "\n",
    "\n",
    "recovery_count = df_declared_and_found\\\n",
    "    .join(df_stations, \"station_id\", \"left\")\\\n",
    "    .where((F.col(\"recovery_date\").isNotNull()) & (F.col(\"station_id\") != \"NOT_A_STATION\"))\\\n",
    "    .groupBy(\"station_name\")\\\n",
    "    .agg(F.count(\"station_name\").alias(\"count_recovery\"))\n",
    "\n",
    "station_count.join(recovery_count, \"station_name\", \"inner\")\\\n",
    "    .withColumn(\"recovery_rate(%)\", 100*F.col(\"count_recovery\")/F.col(\"count_station\"))\\\n",
    "    .select(\"station_name\", \"recovery_rate(%)\")\\\n",
    "    .orderBy(F.desc(\"recovery_rate(%)\"))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final words\n",
    "\n",
    "### 1. Tools choice\n",
    "We decided to go with pyspark in this because of its power in terms of performance in big data processing. Also if there's plan to add machine learning component in the future, it'll be easy to still benefit from spark power because of its MLLIB library. \n",
    "\n",
    "We didn't go for cloud technologies because of the size of data. In the future if the fo data increases substantially we can easily go for some spark clusters such as AWS EMR because our script is cross-platform and can be run on cluster as spark submit job.\n",
    "\n",
    "The data source is updated daily. However because we are doing some adhoc market research we can afford daily, weekly or monthly updates of the data lake.\n",
    "> Note that the public API where the data source is exposed has daily API calls limit. So a particular attention is needed if daily update is chosen.\n",
    "\n",
    "### 2. Alternative scenarios\n",
    "* The data is increased by 100x : then we can switch to a cloud platform such as AWS EMR with some auto-scaling properties. This will allow to absorb the data size increase. We'll use a storage such as S3 because of its cost-efficiency.\n",
    "* The data populates a dashboard that must be updated on a daily basis by 7am every day : here we need a scheduling capability. Therefore a tool such as Apache Airflow is suitable for this purpose. We'll have a script that runs peridoically to download the data on a daily basis from the API and process it with our spark cluster.\n",
    "* The database needed to be accessed by 100+ people : We can use a distributed database to increase query performance as the number of users (thus number of queries) increases substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
